{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7329286,"sourceType":"datasetVersion","datasetId":4254359},{"sourceId":7329319,"sourceType":"datasetVersion","datasetId":4254384},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q einops\n!pip install cohere\n!pip install accelerate\n!pip install bitsandbytes\n\n#import\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, GenerationConfig, TextStreamer, LlamaForCausalLM, LlamaTokenizer\n\nimport torch\nfrom inspect import cleandoc\nimport cohere","metadata":{"execution":{"iopub.status.busy":"2024-01-04T00:06:34.915428Z","iopub.execute_input":"2024-01-04T00:06:34.915792Z","iopub.status.idle":"2024-01-04T00:07:44.944160Z","shell.execute_reply.started":"2024-01-04T00:06:34.915763Z","shell.execute_reply":"2024-01-04T00:07:44.943169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function\n#TODO AGGIUNGERE PLOT E DIFFERENZE + REFACTORING\ndef trova_elemento_piu_comune(array_di_coppie):\n    # Unisci tutte le coppie in un unico elenco\n    tutti_elementi = [elemento for coppia in array_di_coppie for elemento in coppia]\n\n    # Conta le occorrenze di ciascun elemento\n    conteggio_elementi = Counter(tutti_elementi)\n\n    # Trova l'elemento più comune\n    elemento_piu_comune, conteggio_max = conteggio_elementi.most_common(1)[0]\n\n    return elemento_piu_comune, conteggio_max\n\ndef elimina_coppie_con_elemento(array_di_coppie, elemento_da_elim):\n    # Utilizza una list comprehension per filtrare le coppie che non contengono l'elemento da eliminare\n    nuove_coppie = [coppia for coppia in array_di_coppie if elemento_da_elim not in coppia]\n\n    return nuove_coppie\n\ndef elimina_elemento_coppie(indices2,to_remove):\n    for pair in indices2:\n        to_remove.append(pair[0])\n    indices2.clear()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:02:26.857419Z","iopub.execute_input":"2024-01-03T12:02:26.857993Z","iopub.status.idle":"2024-01-03T12:02:26.865440Z","shell.execute_reply.started":"2024-01-03T12:02:26.857964Z","shell.execute_reply":"2024-01-03T12:02:26.864377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lettura dataset originale + aggiunta id\ndf = pd.read_csv('/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv')\ndf['id'] = range(0, len(df))\ndf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-03T12:02:26.866765Z","iopub.execute_input":"2024-01-03T12:02:26.867066Z","iopub.status.idle":"2024-01-03T12:02:29.259508Z","shell.execute_reply.started":"2024-01-03T12:02:26.867031Z","shell.execute_reply":"2024-01-03T12:02:29.258559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# conteggio label dataset\nlabel_counts = df['label'].value_counts()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=label_counts.index, y=label_counts.values)\nplt.title('Hist')\nplt.xlabel('Generated')\nplt.ylabel('Count')\nplt.show()\n\nprint(label_counts)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:02:29.261545Z","iopub.execute_input":"2024-01-03T12:02:29.261832Z","iopub.status.idle":"2024-01-03T12:02:29.537187Z","shell.execute_reply.started":"2024-01-03T12:02:29.261807Z","shell.execute_reply":"2024-01-03T12:02:29.536322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# per ogni prompt contiamo le label\nprompt_count = df.groupby(['prompt_name', 'label']).size().unstack()\nprompt_count\n\nprompt_df = pd.DataFrame(columns=['prompt_name', 'human', 'ai'])\ngrouped_df = df.groupby('prompt_name')['label'].value_counts().unstack().reset_index()\n\n# Assegnare i risultati al nuovo DataFrame\nprompt_df['prompt_name'] = grouped_df['prompt_name']\nprompt_df['human'] = grouped_df[0]\nprompt_df['ai'] = grouped_df[1]\n\nprompt_df","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:02:29.538427Z","iopub.execute_input":"2024-01-03T12:02:29.538704Z","iopub.status.idle":"2024-01-03T12:02:29.577421Z","shell.execute_reply.started":"2024-01-03T12:02:29.538679Z","shell.execute_reply":"2024-01-03T12:02:29.576586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similarità tra i testi umani e individuazione id da eliminare\n\nids_to_remove = []\n\n#qui ci stanno i prompt che contengono similarità tra i testi umani\n#prompt_similarity = ['Car-free cities','Does the electoral college work?','Phones and driving']\n\nfor prompt in prompt_df['prompt_name'].unique():\n    # Filter the dataframe based on label and prompt_name\n    filtered_df = df.loc[(df['label'] == 0) & (df['prompt_name'] == prompt), ['text', 'id']]\n\n    # Create a TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n\n    # Fit and transform the text data\n    tfidf_matrix = vectorizer.fit_transform(filtered_df['text'])\n\n    # Calculate cosine similarity matrix\n    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n    # Create a DataFrame with cosine similarity values\n    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, columns=filtered_df['id'], index=filtered_df['id'])\n\n    # Set a threshold\n    threshold = 0.9\n\n    # Find indices of pairs with similarity greater than the threshold\n    indices = [(filtered_df.index[i], filtered_df.index[j]) for i in range(len(cosine_sim_df))\n               for j in range(i + 1, len(cosine_sim_df.columns))\n               if cosine_sim_df.iloc[i, j] > threshold]\n    \n    while len(indices)>0:\n        da_eliminare, count = trova_elemento_piu_comune(indices)\n        if count > 1:\n            indices = elimina_coppie_con_elemento(indices,da_eliminare)\n            ids_to_remove.append(da_eliminare)\n        else:\n            elimina_elemento_coppie(indices,ids_to_remove)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:02:29.578543Z","iopub.execute_input":"2024-01-03T12:02:29.578808Z","iopub.status.idle":"2024-01-03T12:16:52.846316Z","shell.execute_reply.started":"2024-01-03T12:02:29.578785Z","shell.execute_reply":"2024-01-03T12:16:52.845493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# eliminiamo gli id umani con alta similarità\ndf = df[~df['id'].isin(ids_to_remove)]","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:52.847570Z","iopub.execute_input":"2024-01-03T12:16:52.847862Z","iopub.status.idle":"2024-01-03T12:16:52.857052Z","shell.execute_reply.started":"2024-01-03T12:16:52.847836Z","shell.execute_reply":"2024-01-03T12:16:52.856190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# conteggio label dataset ripulito\nlabel_counts = df['label'].value_counts()\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x=label_counts.index, y=label_counts.values)\nplt.title('Hist')\nplt.xlabel('Generated')\nplt.ylabel('Count')\nplt.show()\n\nprint(label_counts)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:52.858105Z","iopub.execute_input":"2024-01-03T12:16:52.858414Z","iopub.status.idle":"2024-01-03T12:16:53.097395Z","shell.execute_reply.started":"2024-01-03T12:16:52.858388Z","shell.execute_reply":"2024-01-03T12:16:53.096494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# per ogni prompt contiamo le label dal dataset ripulito\nprompt_count = df.groupby(['prompt_name', 'label']).size().unstack()\nprompt_count\n\nprompt_df = pd.DataFrame(columns=['prompt_name', 'human', 'ai'])\ngrouped_df = df.groupby('prompt_name')['label'].value_counts().unstack().reset_index()\n\n# Assegnare i risultati al nuovo DataFrame\nprompt_df['prompt_name'] = grouped_df['prompt_name']\nprompt_df['human'] = grouped_df[0]\nprompt_df['ai'] = grouped_df[1]\n\nprompt_df","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:53.098963Z","iopub.execute_input":"2024-01-03T12:16:53.099330Z","iopub.status.idle":"2024-01-03T12:16:53.131271Z","shell.execute_reply.started":"2024-01-03T12:16:53.099284Z","shell.execute_reply":"2024-01-03T12:16:53.130513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TODO Similarità AI","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:53.134859Z","iopub.execute_input":"2024-01-03T12:16:53.135138Z","iopub.status.idle":"2024-01-03T12:16:53.139285Z","shell.execute_reply.started":"2024-01-03T12:16:53.135113Z","shell.execute_reply":"2024-01-03T12:16:53.138417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"treshold = 500\n# prompt che hanno bisogno di generazione di nuovi testi con label 1\nto_generate = prompt_df[prompt_df['ai'] + treshold < prompt_df['human']]\nto_generate['difference'] = to_generate['human'] - to_generate['ai']\nto_generate","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:53.140328Z","iopub.execute_input":"2024-01-03T12:16:53.140647Z","iopub.status.idle":"2024-01-03T12:16:53.158588Z","shell.execute_reply.started":"2024-01-03T12:16:53.140622Z","shell.execute_reply":"2024-01-03T12:16:53.157510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# controlliamo lunghezza testi (ci servirà per la generazione)\ndf['text_length'] = df['text'].str.len()\ntext_len_mean = int(df.query(\"label == 0\")['text_length'].mean())\ntext_len_std = int(df.query(\"label == 0\")['text_length'].std())\n\nprint(f\"Mean length of train essays by human in our dataset: {text_len_mean}\")\nprint(f\"Mean standard deviation of train essays by human in our dataset: {text_len_std}\")\n\ndf = df.drop('text_length', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:53.160105Z","iopub.execute_input":"2024-01-03T12:16:53.160909Z","iopub.status.idle":"2024-01-03T12:16:53.215542Z","shell.execute_reply.started":"2024-01-03T12:16:53.160876Z","shell.execute_reply":"2024-01-03T12:16:53.214703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('/kaggle/working/daigt-v2-train-dataset_step1.csv', index=False)\nto_generate.to_csv('/kaggle/working/to_generate.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:53.216767Z","iopub.execute_input":"2024-01-03T12:16:53.217055Z","iopub.status.idle":"2024-01-03T12:16:57.433469Z","shell.execute_reply.started":"2024-01-03T12:16:53.217030Z","shell.execute_reply":"2024-01-03T12:16:57.432601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/daigt-step1/daigt-v2-train-dataset_step1.csv')\nto_generate = pd.read_csv('/kaggle/input/to-generate/to_generate.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T00:08:38.163285Z","iopub.execute_input":"2024-01-04T00:08:38.164603Z","iopub.status.idle":"2024-01-04T00:08:40.080166Z","shell.execute_reply.started":"2024-01-04T00:08:38.164566Z","shell.execute_reply":"2024-01-04T00:08:40.079393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_generate","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:59.269443Z","iopub.execute_input":"2024-01-03T12:16:59.269744Z","iopub.status.idle":"2024-01-03T12:16:59.280305Z","shell.execute_reply.started":"2024-01-03T12:16:59.269718Z","shell.execute_reply":"2024-01-03T12:16:59.279425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for prompt in df['prompt_name'].unique():\n    prompt_data = df[df['prompt_name'] == prompt]\n    counts = prompt_data['label'].value_counts()\n\n    # Creazione dell'istogramma\n    plt.bar(counts.index, counts.values, tick_label=['Human', 'AI'])\n    plt.title(f'Prompt: {prompt}')\n    plt.xlabel('Generated by')\n    plt.ylabel('Count')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:16:59.281554Z","iopub.execute_input":"2024-01-03T12:16:59.281898Z","iopub.status.idle":"2024-01-03T12:17:02.445274Z","shell.execute_reply.started":"2024-01-03T12:16:59.281866Z","shell.execute_reply":"2024-01-03T12:17:02.444350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2024-01-04T00:09:00.478394Z","iopub.execute_input":"2024-01-04T00:09:00.479248Z","iopub.status.idle":"2024-01-04T00:09:00.483360Z","shell.execute_reply.started":"2024-01-04T00:09:00.479214Z","shell.execute_reply":"2024-01-04T00:09:00.482308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CohereGenerator:\n    def __init__(self):\n        self.apiKey = \"UVnfAQAMgcewstWsT3jqph91WpPs1KYTfVnAamJF\"\n       # with open('/kaggle/input/zanzares/secret.txt', 'r') as file:\n    # Legge il file riga per riga\n        #    for riga in file:\n         #       self.apiKey = riga.strip().split(\"=\")[1]\n        self.co = cohere.Client(f'{self.apiKey}')\n    def generate_text(self, item_to_generate: int, prompt: str):\n        responses = []\n        for _ in range(item_to_generate):\n            response = self.co.generate(\n              model='command-light',\n              prompt=f'''You are a student working on the following assignment: write a text that talks about {prompt}.\n                            Use natural expressions similar to humans.\n                            Vary sentence structure to avoid monotony.\n                            Include relevant personal details about the topic.\n                            Use metaphors and analogies to explain concepts.\n                            Respect the appropriate tone and formality.\n                            Consider the audience and adapt the writing style.\n                            Avoid direct or implicit references to AI identity.\n                            Correct grammatical and stylistic errors.\n                            Try to be not repetitive.\n                            Ensure logical connections between ideas.\n                            Use empathetic and sensitive language when needed.''',\n              max_tokens=1024,\n              temperature=0.8,\n              presence_penalty=0.2)\n            responses.append(response.generations[0].text)\n            time.sleep(15)\n        return responses","metadata":{"execution":{"iopub.status.busy":"2024-01-04T00:09:04.125415Z","iopub.execute_input":"2024-01-04T00:09:04.126019Z","iopub.status.idle":"2024-01-04T00:09:04.133778Z","shell.execute_reply.started":"2024-01-04T00:09:04.125992Z","shell.execute_reply":"2024-01-04T00:09:04.132828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MistralGenerator:\n    def __init__(self):\n        self.model_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n        self.bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                load_in_4bit=True,\n                quantization_config=self.bnb_config,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n                trust_remote_code=True,\n            )\n    def generate_essay(self, prompt: str):\n        messages = [{\n            \"role\":\"user\",\n            \"content\": prompt\n        }]\n        model_inputs = self.tokenizer.apply_chat_template(messages, return_tensors = \"pt\").to('cuda')\n        generated_ids = self.model.generate(\n            model_inputs,\n            max_new_tokens = 7500,\n            do_sample = True,\n            pad_token_id = self.tokenizer.eos_token_id\n        )     \n        decoded = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n        text = decoded[0].split(\"[/INST]\")[1]\n        return text\n        \n    def generate_text(self, item_to_generate: int, prompt: str):\n        texts=[]\n        for _ in range(item_to_generate):\n            prompt_combined = f'''You are a student working on the following assignment: write a text that talks about {prompt}\n                            Instructions: \n                            Use natural expressions similar to humans.\n                            Vary sentence structure to avoid monotony.\n                            Include relevant personal details about the topic.\n                            Use metaphors and analogies to explain concepts.\n                            Respect the appropriate tone and formality.\n                            Consider the audience and adapt the writing style.\n                            Avoid direct or implicit references to AI identity.\n                            Correct grammatical and stylistic errors.\n                            Try to be not repetitive.\n                            Ensure logical connections between ideas.\n                            Use empathetic and sensitive language when needed.'''\n            texts.append(self.generate_essay(prompt_combined).strip())\n        return texts","metadata":{"execution":{"iopub.status.busy":"2024-01-04T00:09:14.480592Z","iopub.execute_input":"2024-01-04T00:09:14.481326Z","iopub.status.idle":"2024-01-04T00:09:14.491247Z","shell.execute_reply.started":"2024-01-04T00:09:14.481280Z","shell.execute_reply":"2024-01-04T00:09:14.490306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cohereGenerator = CohereGenerator()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T00:09:24.268690Z","iopub.execute_input":"2024-01-04T00:09:24.269392Z","iopub.status.idle":"2024-01-04T00:09:24.273326Z","shell.execute_reply.started":"2024-01-04T00:09:24.269362Z","shell.execute_reply":"2024-01-04T00:09:24.272235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in to_generate.iterrows():\n    if(row['difference']> 1000):\n        prompt_name=row['prompt_name']\n        itemsForGenerator = row['difference']//4\n        texts = cohereGenerator.generate_text(itemsForGenerator, prompt_name)\n        data = {'text': texts,\n            'label': [1] * len(texts),\n            'prompt_name': [prompt_name] * len(texts),\n            'source': ['cohere'] * len(texts)}\n        cohere_df = pd.DataFrame(data)\n        cohere_df.to_csv(f'/kaggle/working/cohere_{prompt_name}.csv', index=False)\n        df = pd.concat([df,cohere_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:17:02.484380Z","iopub.execute_input":"2024-01-03T12:17:02.484708Z","iopub.status.idle":"2024-01-03T12:17:28.833083Z","shell.execute_reply.started":"2024-01-03T12:17:02.484675Z","shell.execute_reply":"2024-01-03T12:17:28.831561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('/kaggle/working/daigt-v2-train-dataset_step2_cohere.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:17:28.834342Z","iopub.status.idle":"2024-01-03T12:17:28.834870Z","shell.execute_reply.started":"2024-01-03T12:17:28.834622Z","shell.execute_reply":"2024-01-03T12:17:28.834648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mistralGenerator = MistralGenerator()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T00:09:49.088289Z","iopub.execute_input":"2024-01-04T00:09:49.088675Z","iopub.status.idle":"2024-01-04T00:12:26.788022Z","shell.execute_reply.started":"2024-01-04T00:09:49.088646Z","shell.execute_reply":"2024-01-04T00:12:26.786988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in to_generate.iterrows():\n    if(row['difference']> 1000):\n        prompt_name=row['prompt_name']\n        itemsForGenerator = row['difference']//4\n        texts = mistralGenerator.generate_text(itemsForGenerator, prompt_name)\n        data = {'text': texts,\n            'label': [1] * len(texts),\n            'prompt_name': [prompt_name] * len(texts),\n            'source': ['mistral'] * len(texts)}\n        mistral_df = pd.DataFrame(data)\n        mistral_df.to_csv(f'/kaggle/working/mistral_{prompt_name}.csv', index=False)\n        df = pd.concat([df,mistral_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:22:05.076640Z","iopub.execute_input":"2024-01-03T12:22:05.076960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('/kaggle/working/daigt-v3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T12:17:28.841883Z","iopub.status.idle":"2024-01-03T12:17:28.842334Z","shell.execute_reply.started":"2024-01-03T12:17:28.842094Z","shell.execute_reply":"2024-01-03T12:17:28.842114Z"},"trusted":true},"execution_count":null,"outputs":[]}]}