{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7236767,"sourceType":"datasetVersion","datasetId":4190846},{"sourceId":7267586,"sourceType":"datasetVersion","datasetId":4212715},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q einops\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, GenerationConfig, TextStreamer\n\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-01-02T11:29:42.359076Z","iopub.execute_input":"2024-01-02T11:29:42.359477Z","iopub.status.idle":"2024-01-02T11:29:54.238394Z","shell.execute_reply.started":"2024-01-02T11:29:42.359449Z","shell.execute_reply":"2024-01-02T11:29:54.237412Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"MODEL_NAME_PHI2 = \"microsoft/phi-2\"\n\nmodel_phi2 = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME_PHI2,\n    torch_dtype=\"auto\",\n    flash_attn=True,\n    flash_rotary=True,\n    fused_dense=True,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer_phi2 = AutoTokenizer.from_pretrained(MODEL_NAME_PHI2, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T11:30:04.689538Z","iopub.execute_input":"2024-01-02T11:30:04.690145Z","iopub.status.idle":"2024-01-02T11:30:08.254572Z","shell.execute_reply.started":"2024-01-02T11:30:04.690110Z","shell.execute_reply":"2024-01-02T11:30:08.253713Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26c2395077c84b6bbf303d7faa6cbba8"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_config = GenerationConfig.from_pretrained(MODEL_NAME_PHI2)\ngeneration_config.max_new_tokens = 1024\ngeneration_config.temperature = 0.0001\ngeneration_config.do_sample = True\n\nstreamer = TextStreamer(tokenizer_phi2, skip_prompt=True, skip_special_tokens=True)\n\nllm = pipeline(\n    \"text-generation\",\n    model=model_phi2,\n    tokenizer=tokenizer_phi2,\n    return_full_text=True,\n    generation_config=generation_config,\n    num_return_sequences=1,\n    eos_token_id=tokenizer_phi2.eos_token_id,\n    pad_token_id=tokenizer_phi2.eos_token_id,\n    streamer=streamer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T11:30:14.307640Z","iopub.execute_input":"2024-01-02T11:30:14.308009Z","iopub.status.idle":"2024-01-02T11:30:14.421775Z","shell.execute_reply.started":"2024-01-02T11:30:14.307971Z","shell.execute_reply":"2024-01-02T11:30:14.420826Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from inspect import cleandoc\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T11:31:43.019979Z","iopub.execute_input":"2024-01-02T11:31:43.020694Z","iopub.status.idle":"2024-01-02T11:31:43.024723Z","shell.execute_reply.started":"2024-01-02T11:31:43.020659Z","shell.execute_reply":"2024-01-02T11:31:43.023795Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"\nYou are a student working on the following assignment: write a text that talks about, phone and driving.\n\"\"\".strip()\n\n\ndef create_prompt(prompt: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n    if not system_prompt:\n        return cleandoc(\n            f\"\"\"\n        Instruct: {prompt}\n        Output:\n        \"\"\"\n        )\n    return cleandoc(\n        f\"\"\"\n        Instruct: {system_prompt} {prompt}\n        Output:\n        \"\"\"\n    )\n     ","metadata":{"execution":{"iopub.status.busy":"2024-01-02T11:31:45.904076Z","iopub.execute_input":"2024-01-02T11:31:45.904465Z","iopub.status.idle":"2024-01-02T11:31:45.910722Z","shell.execute_reply.started":"2024-01-02T11:31:45.904433Z","shell.execute_reply":"2024-01-02T11:31:45.909818Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"output = llm(create_prompt(f'''Use natural expressions similar to humans.\n                            Vary sentence structure to avoid monotony.\n                            Include relevant personal details about the topic.\n                            Use metaphors and analogies to explain concepts.\n                            Respect the appropriate tone and formality.\n                            Consider the audience and adapt the writing style.\n                            Avoid direct or implicit references to AI identity.\n                            Correct grammatical and stylistic errors.\n                            Try to be not repetitive.\n                            Ensure logical connections between ideas.\n                            Use empathetic and sensitive language when needed.'''))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T11:32:17.172228Z","iopub.execute_input":"2024-01-02T11:32:17.172847Z","iopub.status.idle":"2024-01-02T11:32:26.432685Z","shell.execute_reply.started":"2024-01-02T11:32:17.172812Z","shell.execute_reply":"2024-01-02T11:32:26.431880Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\n                    \"I know it's tempting to pick up the phone and answer it while driving, but it's like trying to juggle flaming torches while riding a unicycle. It's just not worth the risk. I remember when I was younger, my dad used to tell me stories about how he almost got into a serious accident because he was texting while driving. It's not just about the danger to yourself, but also to others on the road. It's like playing Russian roulette with people's lives. So, let's all make a conscious effort to put our phones away while driving and focus on the road. It's not that hard, really. Just like how we learned to ride a bike or tie our shoelaces, it's a skill that we can all master with practice. And who knows, maybe one day we'll be able to teach our AI friends how to drive safely too!\"\n\n","output_type":"stream"}]}]}